{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1547g0wfX9H"
   },
   "source": [
    "# \n",
    "# <center> CBERS-4A IMAGERY FOR MAPPING URBAN LAND COVER IN THE AMAZON  </center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Bruno Dias dos Santos, Michelle Azevedo, Luisa Akemi, Carolina Moutinho Duque de Pinho, Antonio Paez and Silvana Amaral  \n",
    "    <br/><br/>\n",
    "    National Institute for Space Research (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    e-mail to: <div><a href=\"mailto:bruno.santos@inpe.br\">bruno.santos@inpe.br</a></div>\n",
    "    <br/><br/>\n",
    "</div>\n",
    "<br/>\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Abstract.</b> The Brazilian Institute of Geography and Statistics (IBGE) identified Intraurban Typologies in several Brazilian urban concentrations using microdata from the 2010 Census. However, using weighting areas as a spatial unit of analysis and the indicators adopted makes it challenging to replicate the methodology for other areas. This paper aims to adapt the IBGE study to the reality of an Amazonian city, choosing Santarém as study site. To achieve this objective, we adapted the socioeconomic indicators to the Amazonian context. As a result, we identified  six intra-urban typologies through an unsupervised classification, which differ concerning  population profile, housing conditions and location in the study area.\n",
    "    \n",
    "Key words — Land cover, GEOBIA, Random Forest, CBERS-4A, Data Mining.\n",
    "\n",
    "</div>    \n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1º Etapa: Tratamento e seleção de variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ8yYaoafX9T"
   },
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFJxn1XjfX9V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import folium\n",
    "import mapclassify\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score,ConfusionMatrixDisplay,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtcvr1cbfX9Y"
   },
   "source": [
    "Reading the segments with attributes and creating a GeoDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w34sr0nHfX9Z"
   },
   "outputs": [],
   "source": [
    "obj = gpd.read_file(\"E:\\\\00_INPE\\\\PDI\\\\TRABALHO-FINAL\\\\cameta\\\\FINAIS\\\\classificado\\\\nivel1_cameta.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "o_ieAwI5fX9k",
    "outputId": "c36abef2-603d-41dd-82cb-f7318289ebab"
   },
   "outputs": [],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQq3gX-2fX9l"
   },
   "source": [
    "Defining the identifier variable of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhmXxHw3fX9l"
   },
   "outputs": [],
   "source": [
    "indice = 'DN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3q0Af6NfX9m"
   },
   "source": [
    "Defining the variable with the sample classes. This variable must be categorical or discrete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP_8iN0pfX9m",
    "outputId": "5ab687cb-4888-4c93-db47-457a6a6fd1da"
   },
   "outputs": [],
   "source": [
    "TARGET = 'TARGET'\n",
    "\n",
    "target = []\n",
    "cont = 0\n",
    "\n",
    "for classe in obj[TARGET].unique():\n",
    "    if not pd.isnull(classe):\n",
    "        target.append(classe)\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the 'index' and 'TARGET' information from the original base to a new GeoDataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = obj[[indice,'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1I43Zs4fX9n"
   },
   "source": [
    "Extracting categorical and numerical variables for the data treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRywbNoifX9o"
   },
   "outputs": [],
   "source": [
    "var_num = pd.DataFrame(obj.select_dtypes(include=['float64','int64','int','float']))\n",
    "var_cat = pd.DataFrame(obj.select_dtypes(include=['string','object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee9X-xolfX9o"
   },
   "source": [
    "Removing the sample column for the data treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKEZkOUofX9p"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    var_cat = var_cat.drop(columns = TARGET)\n",
    "except:\n",
    "    var_num = var_num.drop(columns = TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wm92SBzZObyx"
   },
   "source": [
    "Removendo outliers dos campos numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tEAJLObOjWc"
   },
   "outputs": [],
   "source": [
    "#Function for removing outliers by considering as outlier values greater than 2.698 σ (standard deviation) of the normal distribution curve: \n",
    "\n",
    "def rmv_outliers(DataFrame, col_name):\n",
    "    intervalo = 2.698 * DataFrame[col_name].std()\n",
    "    media = DataFrame[col_name].mean()\n",
    "    DataFrame.loc[DataFrame[col_name] < (media - intervalo), col_name] = np.nan\n",
    "    DataFrame.loc[DataFrame[col_name] > (media + intervalo), col_name] = np.nan\n",
    "\n",
    "for coluna in var_num.columns:\n",
    "    rmv_outliers(var_num, coluna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhZluQAgfX9q"
   },
   "source": [
    "Normalizing and filling empty values in numeric fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "98bk0CEofX9q",
    "outputId": "a83e184d-9cae-44ed-c809-1cd349acfeee"
   },
   "outputs": [],
   "source": [
    "#dummy = var_num.iloc[:,1:].mean()\n",
    "dummy = 0\n",
    "\n",
    "var_num.iloc[:,1:] = var_num.iloc[:,1:].fillna(dummy)\n",
    "\n",
    "var_num.iloc[:,1:] =(var_num.iloc[:,1:] - var_num.iloc[:,1:].min())/(var_num.iloc[:,1:].max() - var_num.iloc[:,1:].min())\n",
    "var_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ERemRNfX9r"
   },
   "source": [
    "Applying OneHotEncoder to variables of a categorical type and creating a DataFrame of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "w7MCAIOZfX9r",
    "outputId": "a356fba4-ed01-4e4f-d805-7a7ec4e4eba7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aux = obj[[indice, TARGET]]\n",
    "\n",
    "try:\n",
    "    var_cat = pd.get_dummies(var_cat[:-1], drop_first=True)\n",
    "    obj = (aux.merge(var_num, left_on=indice, right_on=indice)).merge(var_cat, left_index=True, right_index=True)\n",
    "    \n",
    "except:\n",
    "    obj = (aux.merge(var_num, left_on=indice, right_on=indice))\n",
    "    print(\"Não há variáveis categóricas para aplicar OneHotEncode\")\n",
    "\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tE72sv5fX9s"
   },
   "source": [
    "Viewing descriptive statistics of the already processed database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "zqIZXFBNfX9t",
    "outputId": "c205a340-2874-456d-853e-7759397bf119"
   },
   "outputs": [],
   "source": [
    "obj.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY02ws2jfX9u"
   },
   "source": [
    "Creating a copy of the already processed DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "AFJL3Ad_fX9u",
    "outputId": "b554c818-7fec-4750-d432-6b1f89a5253f"
   },
   "outputs": [],
   "source": [
    "saida =  obj\n",
    "saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkKsJL8rfX9v"
   },
   "source": [
    "Selecting only the sampled features to extract the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "hUzn-BmxfX9v",
    "outputId": "a3066c5e-cf41-4f20-d292-0fedf9139375",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = obj[obj[TARGET].isin(target)]\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TiFgfEUfX9w"
   },
   "source": [
    "Calculate the R² of the Anova of each column against the Target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIhhkrRdfX9w",
    "outputId": "1bbfc8d7-593c-4764-af6c-9953eefd8664"
   },
   "outputs": [],
   "source": [
    "iv = {}\n",
    "\n",
    "for coluna in obj.columns:  \n",
    "    if coluna != TARGET:\n",
    "        counts = obj.groupby(TARGET, sort=True)[coluna].count() # Count of elements in each sample class\n",
    "        medias = obj.groupby(TARGET, sort=True)[coluna].mean() # Average of each caluna per sample class \n",
    "        aux = 0        \n",
    "        for i in range(len(counts)):\n",
    "            try:\n",
    "                aux = aux + counts.iloc[i]*((medias.iloc[i] - obj[coluna].mean())**2)\n",
    "            except:\n",
    "                aux = 0\n",
    "        \n",
    "        if (sum(counts))*((obj[coluna].std())**2) == 0:\n",
    "            iv[coluna] = aux/0.00001\n",
    "        else:                \n",
    "            iv[coluna] = aux/((sum(counts))*((obj[coluna].std())**2))\n",
    "        \n",
    "        print(\"Rodou: \", coluna)\n",
    "\n",
    "iv = sorted(iv.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhyPoJWBfX9x",
    "outputId": "a3217841-92fa-4683-f18b-4538bb92cba6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brAMPcgvfX9x"
   },
   "source": [
    "Bloxpot plot of the variable with the best explaining power in relation to the sample classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "N7TcXJsGfX9y",
    "outputId": "6cbda036-b67a-403a-a54a-4f4b06a88b0e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.boxplot(x=TARGET , y= obj[iv[0][0]], order= obj[TARGET].sort_values().unique(), data = obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lZ4yLFDfX9y"
   },
   "source": [
    "Bloxpot plot of the variable with the worst explaining power in relation to the sample classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "rgGY3AEHfX9y",
    "outputId": "7c35a701-8039-4c42-ef39-bc62b6d87589"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.boxplot(x=TARGET , y= obj[iv[-1][0]], order= obj[TARGET].sort_values().unique(), data = obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tlzE8UjfX9z"
   },
   "source": [
    "Removing poorly explanatory variables from a threshold and variables with possible 'NaN' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QV8Q91gmfX90",
    "outputId": "f9bb6d74-2ac4-4134-a877-4774dbe969d2"
   },
   "outputs": [],
   "source": [
    "lim_min = 0.1 #Alterar limite inferior para remoção das variáveis\n",
    "aux = []\n",
    "\n",
    "print(f'Total de variáveis antes da remoção: {len(iv)}')\n",
    "\n",
    "for i in range(len(iv)):\n",
    "    if math.isnan(iv[i][1]):\n",
    "        aux.append(iv[i])\n",
    "    elif iv[i][0] != indice:\n",
    "        if iv[i][1] < lim_min:\n",
    "            aux.append(iv[i])\n",
    "\n",
    "for i in aux:\n",
    "    iv.remove(i)\n",
    "\n",
    "print(f'Total de variáveis depois da remoção: {len(iv)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ko1A1vfX90"
   },
   "source": [
    "Viewing the most explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq07K7AmfX90",
    "outputId": "b360e8f5-ab8a-4b2f-e76c-df0c6199813b"
   },
   "outputs": [],
   "source": [
    "iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wmse1RMbfX91"
   },
   "source": [
    "Defining the correlation factor to be considered.\n",
    "\n",
    "Given two variables [i,j], the correlation between i and j will be calculated. If the correlation between the two variables is greater than the maximum correlation factor, we will exclude the one with less explanatory power from its R²:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12H_8XE1fX91"
   },
   "outputs": [],
   "source": [
    "fator = 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEUHEBdKfX91"
   },
   "source": [
    "Removal of highly correlated _(Pearson)_ variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zeluhp-ffX91",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colunas = []\n",
    "aux = []\n",
    "\n",
    "for i, j in iv:\n",
    "    colunas.append(i)\n",
    "    aux.append(i)\n",
    "\n",
    "for i in range(len(colunas)):\n",
    "    for j in range(len(colunas)):\n",
    "        if j > i and abs(saida[colunas[i]].corr(saida[colunas[j]])) > fator:\n",
    "            if (colunas[j] in aux) and (colunas[j] != indice):\n",
    "                aux.remove(colunas[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GI39AA6fX92"
   },
   "source": [
    "Output Dataframe variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8x1ZjF5VfX92",
    "outputId": "4dba8b5b-b350-46fc-ce15-1df3b49573f9"
   },
   "outputs": [],
   "source": [
    "len(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJNEuO_vfX92",
    "outputId": "51413fa2-b207-4ac5-930a-4bff1b215211",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acvB3i66fX93"
   },
   "source": [
    "Visualizing the correlation between variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R3nQbyHhfX93",
    "outputId": "a7a38e1e-7642-491c-8d70-f82d5fd161c2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_df = saida[aux].corr()\n",
    "\n",
    "corr_df.style.background_gradient(cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "10iPGVPt9QJM",
    "outputId": "b9af5719-11f3-4e7f-e5c8-43c5edff9282"
   },
   "outputs": [],
   "source": [
    "corr_df = saida[aux].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr_df, annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q_L3h-DfX93"
   },
   "outputs": [],
   "source": [
    "aux.append(TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a shapefile with the treated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = geom.merge(saida[aux], left_on=indice, right_on=indice)\n",
    "geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.to_file(\"E:\\\\00_INPE\\\\0_DISSERTACAO\\\\VALIDACAO\\\\COBERTURA\\\\segmentos_cobertura_tratados.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervision Classification of the land cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the database with samples to build the supervised classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostras = geom.replace(to_replace='None', value=np.nan).dropna()\n",
    "amostras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(amostras.iloc[:,2:-1])\n",
    "Y = pd.DataFrame(amostras[TARGET]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yei6Xu5wa7PR"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify = Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fnq9fB_Fegc4",
    "outputId": "44dfa591-6e8d-4215-86b7-54d1afde9342"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZHg-ypbezV0",
    "outputId": "52dbdf7a-bec5-4f3f-ddb3-04cf278d17a3"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the sample classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geuM_UQlZT4R",
    "outputId": "80510fca-3c85-45ed-aa08-fee85956156a"
   },
   "outputs": [],
   "source": [
    "amostras[TARGET].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amostras.groupby(TARGET)[indice].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of hyperparameters and values for RandomizedSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {'n_estimators':[1,20,50,100,150,200,250,300,350,400,450,500,550,600,700,800,900,1000,1500,2000],\n",
    "              'criterion':['gini','entropy'],\n",
    "              'max_depth':[5,10,20, None],\n",
    "              'min_samples_split':[2,5,10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the RandomizedSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter = 100, verbose=2, random_state=42, param_distributions = parametros, scoring='f1_macro', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing the Random Forest classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the best combination of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model to predict the validation database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the most used variables in the Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(x_test.columns, modelo.best_estimator_.feature_importances_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing performance metrics of the classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = f1_score(y_test, y_pred, average = 'macro')\n",
    "wei = f1_score(y_test, y_pred, average = 'weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results = {'F1_Score_Macro': macro,\n",
    "             'F1_Score_Weighted': wei,\n",
    "             'Global Acuraccy': accuracy \n",
    "            }\n",
    "\n",
    "pd.DataFrame.from_dict(results, orient='index', dtype=None, columns=['Métricas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Reference': y_test.flatten(), 'Predicted': y_pred}\n",
    "df = pd.DataFrame(data, columns = ['Reference','Predicted'])\n",
    "mc = pd.crosstab(df['Reference'], df['Predicted'], rownames=['Reference'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(mc, annot=True,fmt='g',  cmap = 'YlGnBu',linewidths=.5)\n",
    "plt.title('Matriz de Confusão', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Classificação', fontsize=12)\n",
    "plt.ylabel('Referência',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = []\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1_Score = []\n",
    "\n",
    "for i in range(mc.shape[0]):\n",
    "    TP = mc.iloc[i,i]\n",
    "    FP = mc.iloc[i,:].sum() - TP\n",
    "    FN = mc.iloc[:,i].sum() - TP\n",
    "    TN = mc.sum().sum()-TP-FP-FN\n",
    "    \n",
    "    classe.append(mc.index[i]) \n",
    "    Accuracy.append((TP+TN)/mc.sum().sum())\n",
    "    Precision.append(TP/(TP+FP))\n",
    "    Recall.append(TP/(TP+FN))\n",
    "    F1_Score.append(((2*Precision[i]*Recall[i])/(Precision[i] + Recall[i])))\n",
    "    \n",
    "\n",
    "avaliacao = {'classe': classe,\n",
    "            'Precision': Precision,\n",
    "             'Recall': Recall,\n",
    "             'F1_Score':F1_Score,\n",
    "             'Acuraccy':Accuracy\n",
    "            }\n",
    "       \n",
    "pd.DataFrame(avaliacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting the classification into the database shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom['CLASSIFICACAO'] = modelo.predict(geom.iloc[:,2:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Land cover classification map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.plot(column ='CLASSIFICACAO', legend=True, cmap = 'tab20', categorical=True, legend_kwds={'loc': 'center left', 'bbox_to_anchor':(1,0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.explore(column=\"CLASSIFICACAO\", tooltip=\"CLASSIFICACAO\",tiles=\"CartoDB positron\",\n",
    "             categorical = True, cmap='Set2', style_kwds=dict(color=\"grey\", weight=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the shapefile with classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.to_file(\"E:\\\\00_INPE\\\\0_DISSERTACAO\\\\VALIDACAO\\\\COBERTURA\\\\cobertura_cameta_classificado.shp\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Preparação e seleção de variáveis - Disciplina AI - CAP - INPE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
